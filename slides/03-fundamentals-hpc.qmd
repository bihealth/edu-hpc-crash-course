---
title: "HPC Crash Course"
subtitle: "Fundamentals: HPC"

author:
  - name: Manuel Holtgrewe
    orcid: 0000-0002-3051-1763
    affiliations:
      - ref: bihealth
affiliations:
  - id: bihealth
    name: Berlin Institute of Health at Charit√©
    address: Charit√©platz 1,
    postal-code: 10117
    city: Berlin
    country: Germany
title-slide-attributes:
  data-background-size: contain
  data-background-image: themes/bih/bih_bg_logo.png
format:
  revealjs:
    theme:
      - default
      - themes/bih/theme.scss
    slide-number: true
    navigation-mode: linear
    controls-layout: bottom-right
    controls-tutorial: false
    smaller: true
    self-contained: true
    # Override default "normal" size to get 16:9 aspect ratio.
    width: 1200
    height: 675
    margin: 0.05
    # Enable local plugins.
    revealjs-plugins:
      - plugins/fullscreen
---

## Session Outline

- Introduction to HPC

## What is High-Performance Computing? {.inverse background-color="#70ADC1"}

- Attempt at a definition
- Role of HPC in biomedical and medical research
- Trade-Offs

## Attempt at a Definition: HPC ...

::: {.incremental}

- refers to [advanced computing techniques]{.fragment .highlight-red} & technologies to solve [complex computational problems]{.fragment .highlight-red} efficiently
- involves leveraging [parallel processing]{.fragment .highlight-red}, [large-scale data analysis]{.fragment .highlight-red}, and [specialized hardware]{.fragment .highlight-red}
    - to achieve [high computational performance]{.fragment .highlight-red}
- systems consist of [multiple computing nodes]{.fragment .highlight-red} connected through a [high-speed network]{.fragment .highlight-red}, [working together]{.fragment .highlight-red}
- enables [researchers]{.fragment .highlight-red} to tackle [computationally intensive tasks]{.fragment .highlight-red} that would be infeasible or too time-consuming otherwise
- finds applications in a [wide range of fields]{.fragment .highlight-red}, including scientific research, engineering, data analytics, and machine learning

:::

## HPC in Biomedical Research

::: {.incremental}

- ... plays a crucial role by enabling tackling of computational challenges
- ... allows for analyzing large-scale genomics, proteomics, ..., datasets
    - leading to insights into diseases and potential treatments
- ... facilitates simulations such as protein folding, molecule interactions, etc.
- ... enables the efficient training of large-scale statistical and machine learning models

:::

## Trade-Offs of HPC

::::: {.incremental}

:::: {.columns}

::: {.column width="50%"}
__Advantages__

- fast execution of complex computational tasks
- process and analyze large data sets
- fast and large storage systems
- [MORE POWER]{.fragment .highlight-red} ü¶æ
:::

::: {.column width="50%"}
__Drawbacks__

- learning curve / entry barrier
- usually shared with other users
- expensive to buy/operate
- high power usage/CO<sub>2</sub> footprint ([reference](https://www.bcs.org/articles-opinion-and-research/carbon-footprint-the-not-so-hidden-cost-of-high-performance-computing/))
- ["why is my job killed/crashing/not running?"]{.fragment .highlight-red} üò∂‚Äçüå´Ô∏è
:::

::::

:::::

<hr>

There is no free lunch!

## What is Your Take? ü§∏

::: {.box-top-right .inverse}

__"Blitzlicht" üì∏__

- answer one of the questions
- do not repeat a previous answer

:::

__So far__

- have you benefited from advantages?
- have you suffered from drawbacks?

__From here on__

- what do you hope to gain from using HPC?
- what risks do you see?

## Programming HPC Systems  {.inverse background-color="#70ADC1"}

- There is no Magic or Silver Bullet
- Common Paradigms for Parallel Programing

‚ö†Ô∏è "Warning": just a quick and superficial overview ;-)

## There Is No Magic (1)

:::: {.columns}

::: {.column width="50%"}

![](img/03-fundamentals-hpc/wikipedia-cray-1.jpeg){width="100%"}

Cray-1 1976 @80Mhz [Wikipedia](https://en.wikipedia.org/wiki/History_of_supercomputing#/media/File:Cray-1-deutsches-museum.jpg)

:::

::: {.column width="50%"}

![](img/03-fundamentals-hpc/wikipedia-blue-gene.jpeg){width="100%"}

IBM Blue Gene ~2000-2015 [Wikipedia](https://en.wikipedia.org/wiki/History_of_supercomputing#/media/File:IBM_Blue_Gene_P_supercomputer.jpg)

**"Just a (large) bunch of computers with fast interconnect."**

:::

::::

## There Is No Magic (2)

:::: {.columns}

::: {.column width="50%"}

- From the perspective of 1976, your smartphone is a supercomputer.
- iPhone 14
    - ~1,000EUR
    - 6 cores @3.23Ghz; 6GB RAM
    - GPU: 5-core GPU
    - "Neural Engine": 15.8TOPs

:::

::: {.column width="50%"}

- 2002 NEC Earth Simulator
    - 412,000,000EUR (2023; inflation corrected)
    - 36TFLOPs
- 1976 Cray-1
    - 24,623,570EUR (2023; inflation corrected)
    - 160MFLOPS

:::

::::

## There Is No Magic (3)

- Is it easy to program an iPhone?
    - "yes and no"
- Is it easy to program for an HPC?
    - "yes and no"

## "Es gibt keinen K√∂nigsweg" (1)

::: {.incremental}

- Master using Linux.
- Master programming for
    - a single core
    - multiple cores on a single machine
    - multiple cores on multiple machines
    - programming GPUs
- Realize that most problems are

:::

## "Es gibt keinen K√∂nigsweg" (2)

- Master using Linux.
- Master programming for
    - a single core
    - multiple cores on a single machine
    - ~~multiple cores on multiple machines~~
    - programming GPUs
- Realize that most problems
    - are embarassingly parallel

## "Es gibt keinen K√∂nigsweg" (3)

- Master using Linux.
- Master programming for
    - a single core
    - multiple cores on a single machine
    - ~~multiple cores on multiple machines~~
    - ~~programming GPUs~~
- Realize that most problems
    - are embarassingly parallel
    - have well-solved "cores"

## Types of Parallelism

**Single-core level** (not in focus here)

- Bit-level parallelism
    - aka bit-wise AND, OR, etc.
- Instruction-level parallelism
    - aka instruction pipelining

**Programming level**

- Pipeline parallelism
- Task parallelism
- Data parallelism

## Pipeline Parallelism

:::: {.columns}

::: {.column width="50%"}
![](img/03-fundamentals-hpc/stanford-laundry-1.png)
![](img/03-fundamentals-hpc/stanford-laundry-2.png)
[Source: Stanford](https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/pipelining/index.html)
:::

::: {.column width="50%"}
- Consider an imaginary laundry
    - Laundry steps: wash, dry, ironing
    - One machine/operatore per step
- Naive / sequential execution is slow
- We can speedup the process with pipeline Parallelism
    - The bottleneck / dominating step is **drying**
    - Also: pipeline startup/shutdown
:::

::::


## Task Parallelism (1)

:::: {.columns}

::: {.column width="50%"}
![](img/03-fundamentals-hpc/task-parallelism.svg){width="100%"}
:::

::: {.column width="50%"}
- Often, work can be split into different tasks
- These tasks can be processed independently
- Tasks may have different "sizes" (required RAM, ...)
- Tasks may have dependencies
- If we can easily split (part of) the work into independent tasks:
    - **embarassingly parallel**
:::

::::


## Task Parallelism (2)

:::: {.columns}

::: {.column width="50%"}
![](img/03-fundamentals-hpc/manager-worker.svg){width="80%"}
:::

::: {.column width="50%"}
- How to process these tasks?
- Commonly, the manager-worker pattern is applied.
- The manager
    - has a (potentially dynamic) list of tasks
    - distributes the tasks to the workers
- The workers
    - do the actual work
:::

::::

## Data Parallelism {.even-smaller}

:::: {.columns}

::: {.column width="40%"}
![](img/03-fundamentals-hpc/wikipedia-matrix-multiplication.svg){width=80%}

- specialized hardware examples: GPUs & TPUs
:::

::: {.column width="60%"}
- **Regularly structured data** (e.g., vectors, matrices, tensors) ...
    - ... **have obvious decompositions** ...
    - ... and operations can be easily parallelized.
- Common applications:
    - Linear algebra / graphics
    - "Deep" learning etc.
    - Finite element methods for differential equations
        - (but with a twist!)
:::

::::

## HPC Systems and Architecture  {.inverse background-color="#70ADC1"}

- Compute nodes
- Shared memory vs. distributed memory systems
- Cluster architecture
- Distributed file systems
- Job schedulers and resource management

‚ö†Ô∏è "Warning": just a quick and superficial overview ;-)

## Compute Nodes (1)

"Same-same (as your laptop), but different."

::::: {.columns}

:::: {.column width="50%"}

::: {.incremental}

- 2+ sockets with
    - many-cores CPUs
    - e.g., 2 x 24 x 2 = 96 threads
- high memory (e.g., 200+ GB)
- fast network interface card
    - "legacy": 10GbE (x2)
    - modern: 25GbE (x2)

- local disks
    - HDD or solid state SSD/NVME
:::

::::

:::: {.column .pull-up-150 width="50%"}

::: {.r-stack}
![](img/03-fundamentals-hpc/hpc-node-photo.webp)

![](img/03-fundamentals-hpc/hpc-node-schematics.png){.fragment}
<!-- Figure source: Haarhoff, Daniel, and Lukas Arnold. "Performance analysis and shared memory parallelisation of FDS." Proceedings of Fire and Evacuation Modeling Technical Conference. 2014. -->
:::

::::

:::::

## Compute Nodes (2)

More differences from "consumer-grade" hardware:

::: {.incremental}

- error correcting memory (bit flips are real)
  - [Google in 2009](http://www.cs.toronto.edu/~bianca/papers/sigmetrics09.pdf): 8% of DIMMs have 1+ 1bit errors/year, 0.2% of DIMMs have 1+ 2bit errors/year
- stronger fans
- redundant power control
- redundant disks
:::

::: {.callout-tip title="You are not the admin"}
no root/admin access, no `sudo`
:::

## Shared vs. Distributed Memory {.even-smaller}

::::: {.incremental}

:::: {.columns}

::: {.column width="50%"}
__Shared Memory__

- in-core/multi-threading

```{mermaid}
graph BT
    sq1[thread 1] --> ci(memory address)
    sq2[thread 2] --> ci(memory address)
```

- ‚ûï low overhead
- ‚ûï easy to get started
- ‚ûñ implicit communication, easy to make errors
- ‚ûñ do you __really__ understand your memory model?

:::

::: {.column width="50%"}
__Distributed Memory__

- out-of-core/message-passing

```{mermaid}
graph LR
    sq1[thread 1] -->|How are you?| sq2[thread 2]
    sq2[thread 2] -->|- fine, thank!| sq1[thread 1]
```

- ‚ûï explicit communication, fewer wrong assumptions(?)
- ‚ûï model scales up better for larger systems
- ‚ûñ harder to get started
- ‚ûñ more complex primitives

:::

::::

:::::

## Your Experience? ü§∏

::: {.box-top-right .inverse}

__"Blitzlicht" üì∏__

- answer one of the questions
- do not repeat a previous answer

:::

- have you used shared/distributed<br>
  memory parallelism before?
- what is your experience/hope?

## Cluster Architecture

:::: {.columns}

::: {.column width="50%"}

- head nodes (login/transfer)
- compute nodes
    - generic: cpu
    - specialized: high-mem/gpus
- storage cluster with parallel file system
- scheduler to orchestrate jobs
- __Network/Interconnect__

:::

::: {.column width="50%"}

![](img/03-fundamentals-hpc/cluster-overview.svg){width=80%}

:::

::::

## (Distributed) File Systems

:::: {.columns}

::: {.column width="50%"}

- directory/files correspond to **inode**
    - inode: **fixed-size** data store
- special inodes such as root directory
    - "it's all inodes from there"
- large directories:
    - indirection
    - OS must scan all indirect blocks for an `ls`

- More? [Ext4 Disk Layout](https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout)

:::

::: {.column width="50%"}

![](img/03-fundamentals-hpc/inode-structure-wikipedia.svg){width="100%"}

[Source: Wikipedia](https://commons.wikimedia.org/wiki/File:Ext2-inode.svg)

:::

::::

## Distributed File Systems (2)

__"Same-same (as your laptop), but different."__

- POSIX file system
  - laptop: ext4/XFS/btrfs/ZFS/...
  - distributed: CephFS, GPFS/SpectrumScale, BeeGFS, ...
- POSIX guarantees
  - `sync()` &rarr; visible everywhere
  - `mkdir`/`open()` &rarr; visible everywhere
  - files can be opened by multiple processes
- ... harder to enforce in a distributed (multi-node) setting

## Distributed File Systems (3)

**Best Practices / Do's & Don'ts**

- use modest sized directories (<10k entries)
  - don't create one file per gene (or similar)
  - create sub directories, e.g., `abcdef` &rarr; `ab/cdef`
- don't splurge in file count
  - don't create one file per NGS read (or similar)
- avoid recursive traversal of large structures
  - `ls -lR` will be slow!

## Distributed File Systems (4)

**Best Practices / Do's & Don'ts**

- avoid small reads/writes and random access
  - each I/O operation (IOP) needs to go through the network
  - I/O systems are better at handling larger/sequential reads/writes
- **DO** stream through your files
  - `for each line/record in file: # do work`
- **DO** use Unix `sort`
- **DO** use Unix pipelines rather than temporary files
  - e.g., `seqtk mergepe R1.fq R2.fq | bwa mem ref.fa | samtools sort | samtools view -O out.bam`

## Distributed File Systems (6)

**Best Practices / Do's & Don'ts**

- **DON'T** be afraid of large files
    - 1GB does not qualify as large
    - 1TB files ... do

## Job Scheduler and Resource Management

```{mermaid}
sequenceDiagram
    autonumber
    User-)+Scheduler: sbatch $resource_args jobscript.sh
    Scheduler->>+Scheduler: add job to queue
    User-)+Scheduler: squeue / scontrol show job
    Scheduler-->>+User: job status
    Note right of Scheduler: scheduler loop
    Scheduler-)Compute Node: start job
    Compute Node->>Compute Node: execute job
    Compute Node-)+Scheduler: job complete
```


## Last but not least... {.inverse background-color="#70ADC1"}

We should make sure that you have access to the HPC system...

## Connecting to the HPC

- Windows Users
  - Should follow the [hpc-docs instructions](https://hpc-docs.cubi.bihealth.org/connecting/ssh-client-windows/)
- Linux / Mac Users
  - Should be able to do `ssh -l $USER_{c,m} hpc-login-1.cubi.bihealth.org

## Python Multiprocessing {.inverse background-color="#70ADC1"}

- Introduction
- Reminder: map/apply
- Example
- Caveats

## Introduction

- Python has a `threading` library with low-level primitives
- But why is there a `multiprocessing`? Processes?
    - Python has a global interpreter lock (GIL)
    - (maybe removed at some point but it is there)
    - Access to Python data structures is serialized
    - Multithreading: only while threads are blocked (I/O)
- Multiprocessing to the rescue:
    - **Copy data** to another process
    - This process can do the work

## Reminder: apply/map

- Remember functional programming?
- `map(func, list) -> list`
    - Apply `func` to each element on the `list` to obtain a new list of same size
    - Can be parallelized if there are no side effects
- `apply(func, list)`
    - Apply `func` to each element on the `list`, ignoring results
    - Can be parallelized if there are no side effects

## Thread Pools

- Thread pools are abstractions for parallelism
- We create a pool with `N` threads
- We let the thread pool process collections / lists of work
    - `multiprocessing.Pool()` (process pool ;-))

## Example

```python
import multiprocessing

def func(element):
    # ...

if __name__ == "__main__":
  work = list(range(1_000_000))

  with multiprocessing.Pool(processes=4) as p:
    # force single element chunks
    done_work = p.map(func, work, chunksize=1)

  print(done_work)
```

## Caveats

:::: {.columns}

::: {.column width="50%"}

- Remember the "copy data to processes"?
- The `func` must be serializeable (top-level function!)
- The arguments must be serializeable (int, str, list, dict)
- If you need to pass parameter etc, it might be better to make this part of the work

:::

::: {.column width=50%}


```python
def func(element, a, b):
  # ...

def func2(element_params):
  element, params = element_params
  func(element, **params)

# ...

params = {
  "a": 1,
  "b": 2,
}

real_work = list(range(1_000_000))
work_with_params = [(x, params) for x in real_work]
done_work = p.map(func, work, chunksize=1)
```

:::

::::

## GNU Parallel {.inverse background-color="#70ADC1"}

- Introduction
- Example

## Introduction

- GNU `parallel` is a command line tool that allows you to
    - run a potentially large list of tasks
    - with a fixed number of parallel tasks at the same time
- It is similar to the earlier Python `multiprocessing.ThreadPool`
- [Excellent Tutorial](https://www.gnu.org/software/parallel/parallel_tutorial.html)

## Example

:::: {.columns}

::: {.column width=50%}

```bash
THREADS=4

parallel -t -j $THREADS \
  'md5sum {} >{}.md5' ::: *.bam

parallel -t -j $THREADS \
  'cd {//}; md5sum {/} >{/}.md5' ::: *.bam
```

:::

::: {.column width=50%}

__Placeholders__

- `{}` - whole argument
- `{/}` - basename (filename) of argument
- `{//}` - dirname of argument
- see `man parallel` for more

:::

::::

## HPC Pitfalls {.inverse background-color="#70ADC1"}

- File System Quotas
- Killed out of Memory

## File System Quotas

- Your `home` volume has tight quotas
- Some programs will write a lot there
- Solution: use `work` volume
  ```
  NAME=.cpan
  mkdir -p work/$NAME
  mv $NAME/* work/$NAME
  rmdir $NAME
  ln -sr work/$NAME $NAME
  ```
- E.g., run the above `for NAME in ondemand miniconda3 R Downloads .apptainer .theano .singularity .npm .nextflow .local .debug .cpan .cache .aspera`

## Killed Out Of Memory Jobs

- If you get random `Killed` messages, your job probably needs more memory than available
- In particular:
    - On login nodes, you only have 100MB of RAM!
- Solution: request more memory!
    - More on this in the Slurm session
