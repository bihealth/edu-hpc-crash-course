---
title: "HPC Crash Course"
subtitle: "Fundamentals: HPC"

author:
  - name: Manuel Holtgrewe
    orcid: 0000-0002-3051-1763
    affiliations:
      - ref: bihealth
affiliations:
  - id: bihealth
    name: Berlin Institute of Health at Charit√©
    address: Charit√©platz 1,
    postal-code: 10117
    city: Berlin
    country: Germany
title-slide-attributes:
  data-background-size: contain
  data-background-image: themes/bih/bih_bg_logo.png
format:
  revealjs:
    theme:
      - default
      - themes/bih/theme.scss
    slide-number: true
    navigation-mode: linear
    controls-layout: bottom-right
    controls-tutorial: false
    smaller: true
    self-contained: true
    # Override default "normal" size to get 16:9 aspect ratio.
    width: 1200
    height: 675
    margin: 0.05
    # Enable local plugins.
    revealjs-plugins:
      - plugins/fullscreen
---

## Session Outline

- Introduction to HPC

## What is High-Performance Computing? {.inverse background-color="#70ADC1"}

- Attempt at a definition
- Trade-Offs

## Attempt at a Definition: HPC ...

::: {.incremental}

- refers to [advanced computing techniques]{.fragment .highlight-red} & technologies to solve [complex computational problems]{.fragment .highlight-red} efficiently
- involves leveraging [parallel processing]{.fragment .highlight-red}, [large-scale data analysis]{.fragment .highlight-red}, and [specialized hardware]{.fragment .highlight-red}
    - to achieve [high computational performance]{.fragment .highlight-red}
- systems consist of [multiple computing nodes]{.fragment .highlight-red} connected through a [high-speed network]{.fragment .highlight-red}, [working together]{.fragment .highlight-red}
- enables [researchers]{.fragment .highlight-red} to tackle [computationally intensive tasks]{.fragment .highlight-red} that would be infeasible or too time-consuming otherwise
- finds applications in a [wide range of fields]{.fragment .highlight-red}, including scientific research, engineering, data analytics, and machine learning

:::

## Trade-Offs of HPC

::::: {.incremental}

:::: {.columns}

::: {.column width="50%"}
__Advantages__

- fast execution of complex computational tasks
- process and analyze large data sets
- fast and large storage systems
- [MORE POWER]{.fragment .highlight-red} ü¶æ
:::

::: {.column width="50%"}
__Drawbacks__

- learning curve / entry barrier
- usually shared with other users
- expensive to buy/operate
- high power usage/CO<sub>2</sub> footprint ([reference](https://www.bcs.org/articles-opinion-and-research/carbon-footprint-the-not-so-hidden-cost-of-high-performance-computing/))
- ["why is my job killed/crashing/not running?"]{.fragment .highlight-red} üò∂‚Äçüå´Ô∏è
:::

::::

:::::

<hr>

There is no free lunch!

## Programming HPC Systems  {.inverse background-color="#70ADC1"}

- Common Paradigms for Parallel Programing

‚ö†Ô∏è "Warning": just a quick and superficial overview ;-)

## "Es gibt keinen K√∂nigsweg" (1)

::: {.incremental}

- Master using Linux.
- Master programming for
    - a single core
    - multiple cores on a single machine
    - multiple cores on multiple machines
    - programming GPUs
- Realize that most problems are

:::

## "Es gibt keinen K√∂nigsweg" (2)

- Master using Linux.
- Master programming for
    - a single core
    - multiple cores on a single machine
    - ~~multiple cores on multiple machines~~
    - programming GPUs
- Realize that most problems
    - are embarassingly parallel

## "Es gibt keinen K√∂nigsweg" (3)

- Master using Linux.
- Master programming for
    - a single core
    - multiple cores on a single machine
    - ~~multiple cores on multiple machines~~
    - ~~programming GPUs~~
- Realize that most problems
    - are embarassingly parallel
    - have well-solved "cores"

## Types of Parallelism

**Single-core level** (not in focus here)

- Bit-level parallelism
    - aka bit-wise AND, OR, etc.
- Instruction-level parallelism
    - aka instruction pipelining

**Programming level**

- Pipeline parallelism
- Task parallelism
- Data parallelism

## Pipeline Parallelism

:::: {.columns}

::: {.column width="50%"}
![](img/03-fundamentals-hpc/stanford-laundry-1.png)
![](img/03-fundamentals-hpc/stanford-laundry-2.png)
[Source: Stanford](https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/pipelining/index.html)
:::

::: {.column width="50%"}
- Consider an imaginary laundry
    - Laundry steps: wash, dry, ironing
    - One machine/operatore per step
- Naive / sequential execution is slow
- We can speedup the process with pipeline Parallelism
    - The bottleneck / dominating step is **drying**
    - Also: pipeline startup/shutdown
:::

::::


## Task Parallelism (1)

:::: {.columns}

::: {.column width="50%"}
![](img/03-fundamentals-hpc/task-parallelism.svg){width="100%"}
:::

::: {.column width="50%"}
- Often, work can be split into different tasks
- These tasks can be processed independently
- Tasks may have different "sizes" (required RAM, ...)
- Tasks may have dependencies
- If we can easily split (part of) the work into independent tasks:
    - **embarassingly parallel**
:::

::::


## Task Parallelism (2)

:::: {.columns}

::: {.column width="50%"}
![](img/03-fundamentals-hpc/manager-worker.svg){width="80%"}
:::

::: {.column width="50%"}
- How to process these tasks?
- Commonly, the manager-worker pattern is applied.
- The manager
    - has a (potentially dynamic) list of tasks
    - distributes the tasks to the workers
- The workers
    - do the actual work
:::

::::

## Data Parallelism {.even-smaller}

:::: {.columns}

::: {.column width="40%"}
![](img/03-fundamentals-hpc/wikipedia-matrix-multiplication.svg){width=80%}

- specialized hardware examples: GPUs & TPUs
:::

::: {.column width="60%"}
- **Regularly structured data** (e.g., vectors, matrices, tensors) ...
    - ... **have obvious decompositions** ...
    - ... and operations can be easily parallelized.
- Common applications:
    - Linear algebra / graphics
    - "Deep" learning etc.
    - Finite element methods for differential equations
        - (but with a twist!)
:::

::::

## HPC Systems and Architecture  {.inverse background-color="#70ADC1"}

- Compute nodes
- Cluster architecture
- Distributed file systems
- Job schedulers and resource management

‚ö†Ô∏è "Warning": just a quick and superficial overview ;-)

## Compute Nodes (1)

"Same-same (as your laptop), but different."

::::: {.columns}

:::: {.column width="50%"}

::: {.incremental}

- 2+ sockets with
    - many-cores CPUs
    - e.g., 2 x 24 x 2 = 96 threads
- high memory (e.g., 200+ GB)
- fast network interface card
    - "legacy": 10GbE (x2)
    - modern: 25GbE (x2)

- local disks
    - HDD or solid state SSD/NVME
:::

::::

:::: {.column .pull-up-150 width="50%"}

::: {.r-stack}
![](img/03-fundamentals-hpc/hpc-node-photo.webp)

![](img/03-fundamentals-hpc/hpc-node-schematics.png){.fragment}
<!-- Figure source: Haarhoff, Daniel, and Lukas Arnold. "Performance analysis and shared memory parallelisation of FDS." Proceedings of Fire and Evacuation Modeling Technical Conference. 2014. -->
:::

::::

:::::

## Compute Nodes (2)

More differences from "consumer-grade" hardware:

::: {.incremental}

- error correcting memory (bit flips are real)
  - [Google in 2009](http://www.cs.toronto.edu/~bianca/papers/sigmetrics09.pdf): 8% of DIMMs have 1+ 1bit errors/year, 0.2% of DIMMs have 1+ 2bit errors/year
- stronger fans
- redundant power control
- redundant disks
:::

::: {.callout-tip title="You are not the admin"}
no root/admin access, no `sudo`
:::

## Cluster Architecture

:::: {.columns}

::: {.column width="50%"}

- head nodes (login/transfer)
- compute nodes
    - generic: cpu
    - specialized: high-mem/gpus
- storage cluster with parallel file system
- scheduler to orchestrate jobs
- __Network/Interconnect__

:::

::: {.column width="50%"}

![](img/03-fundamentals-hpc/cluster-overview.svg){width=80%}

:::

::::

## Job Scheduler and Resource Management

```{mermaid}
sequenceDiagram
    autonumber
    User-)+Scheduler: sbatch $resource_args jobscript.sh
    Scheduler->>+Scheduler: add job to queue
    User-)+Scheduler: squeue / scontrol show job
    Scheduler-->>+User: job status
    Note right of Scheduler: scheduler loop
    Scheduler-)Compute Node: start job
    Compute Node->>Compute Node: execute job
    Compute Node-)+Scheduler: job complete
```

## HPC Pitfalls {.inverse background-color="#70ADC1"}

- File System Quotas
- Killed out of Memory

## File System Quotas

- Your `home` volume has tight quotas
- Some programs will write a lot there
- Solution: use `work` volume
  ```
  NAME=.cpan
  mkdir -p work/$NAME
  mv $NAME/* work/$NAME
  rmdir $NAME
  ln -sr work/$NAME $NAME
  ```
- E.g., run the above `for NAME in ondemand miniconda3 R Downloads .apptainer .theano .singularity .npm .nextflow .local .debug .cpan .cache .aspera`

## Killed Out Of Memory Jobs

- If you get random `Killed` messages, your job probably needs more memory than available
- In particular:
    - On login nodes, you only have 100MB of RAM!
- Solution: request more memory!
    - More on this in the Slurm session

## Python Multiprocessing {.inverse background-color="#70ADC1"}

- Introduction
- Reminder: map/apply
- Example
- Caveats

## Introduction

- Python has a `threading` library with low-level primitives
- But why is there a `multiprocessing`? Processes?
    - Python has a global interpreter lock (GIL)
    - (maybe removed at some point but it is there)
    - Access to Python data structures is serialized
    - Multithreading: only while threads are blocked (I/O)
- Multiprocessing to the rescue:
    - **Copy data** to another process
    - This process can do the work

## Reminder: apply/map

- Remember functional programming?
- `map(func, list) -> list`
    - Apply `func` to each element on the `list` to obtain a new list of same size
    - Can be parallelized if there are no side effects
- `apply(func, list)`
    - Apply `func` to each element on the `list`, ignoring results
    - Can be parallelized if there are no side effects

## Thread Pools

- Thread pools are abstractions for parallelism
- We create a pool with `N` threads
- We let the thread pool process collections / lists of work
    - `multiprocessing.Pool()` (process pool ;-))

## Example

```python
import multiprocessing

def func(element):
    # ...

if __name__ == "__main__":
  work = list(range(1_000_000))

  with multiprocessing.Pool(processes=4) as p:
    # force single element chunks
    done_work = p.map(func, work, chunksize=1)

  print(done_work)
```

## Caveats

:::: {.columns}

::: {.column width="50%"}

- Remember the "copy data to processes"?
- The `func` must be serializeable (top-level function!)
- The arguments must be serializeable (int, str, list, dict)
- If you need to pass parameter etc, it might be better to make this part of the work

:::

::: {.column width=50%}


```python
def func(element, a, b):
  # ...

def func2(element_params):
  element, params = element_params
  func(element, **params)

# ...

params = {
  "a": 1,
  "b": 2,
}

real_work = list(range(1_000_000))
work_with_params = [(x, params) for x in real_work]
done_work = p.map(func, work, chunksize=1)
```

:::

::::

## GNU Parallel {.inverse background-color="#70ADC1"}

- Introduction
- Example

## Introduction

- GNU `parallel` is a command line tool that allows you to
    - run a potentially large list of tasks
    - with a fixed number of parallel tasks at the same time
- It is similar to the earlier Python `multiprocessing.ThreadPool`
- [Excellent Tutorial](https://www.gnu.org/software/parallel/parallel_tutorial.html)

## Example

:::: {.columns}

::: {.column width=50%}

```bash
THREADS=4

parallel -t -j $THREADS \
  'md5sum {} >{}.md5' ::: *.bam

parallel -t -j $THREADS \
  'cd {//}; md5sum {/} >{/}.md5' ::: *.bam
```

:::

::: {.column width=50%}

__Placeholders__

- `{}` - whole argument
- `{/}` - basename (filename) of argument
- `{//}` - dirname of argument
- see `man parallel` for more

:::

::::
